{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for sentiment classify and export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert basic module\n",
    "import tokenization\n",
    "import optimization\n",
    "import run_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_model_hub):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_model_hub)\n",
    "        tokenization_info = bert_module(\n",
    "            signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN):\n",
    "    input_example = dataset.apply(lambda x: run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "    features = run_classifier.convert_examples_to_features(input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "        bert_model_hub,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "#         predicted_labels = tf.squeeze(\n",
    "#             tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "#         if is_predicting:\n",
    "#             return (predicted_labels, probabilities)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "#         return (loss, predicted_labels, log_probs)\n",
    "        return (loss, per_example_loss, logits, probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(bert_model_hub, num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        \n",
    "        (loss, per_example_loss, logits, probabilities) = create_model(\n",
    "                bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "#             (loss, predicted_labels, log_probs) = create_model(\n",
    "#                 bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            train_op = optimization.create_optimizer(\n",
    "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)            \n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                  loss=loss,\n",
    "                                                  train_op=train_op)\n",
    "            else:\n",
    "                def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
    "                    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                    accuracy = tf.metrics.accuracy(labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "                    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "                    return {\n",
    "                        \"eval_accuracy\": accuracy,\n",
    "                        \"eval_loss\": loss,\n",
    "                    }\n",
    "                is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "                eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                  loss=loss,\n",
    "                                                  eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "#             (predicted_labels, log_probs) = create_model(\n",
    "#                 bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "#             predicted_labels = tf.squeeze(tf.argmax(probabilities, axis=-1, output_type=tf.int32))\n",
    "\n",
    "            predictions = {\n",
    "                'probabilities': probabilities,\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE, num_train_steps, num_warmup_steps, BATCH_SIZE):\n",
    "\n",
    "    # Specify outpit directory and number of checkpoint steps to save\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        model_dir=OUTPUT_DIR,\n",
    "        save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "    model_fn = model_fn_builder(\n",
    "      bert_model_hub = bert_model_hub,\n",
    "      num_labels=len(label_list),\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      params={\"batch_size\": BATCH_SIZE})\n",
    "    return estimator, model_fn, run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_dfs(train, test, DATA_COLUMN, LABEL_COLUMN, \n",
    "               MAX_SEQ_LENGTH = 128,\n",
    "              BATCH_SIZE = 16,\n",
    "              LEARNING_RATE = 2e-5,\n",
    "              NUM_TRAIN_EPOCHS = 3.0,\n",
    "              WARMUP_PROPORTION = 0.1,\n",
    "              SAVE_SUMMARY_STEPS = 100,\n",
    "              SAVE_CHECKPOINTS_STEPS = 10000,\n",
    "              bert_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"):\n",
    "\n",
    "    label_list = sorted(train[LABEL_COLUMN].unique().tolist())\n",
    "    \n",
    "    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n",
    "\n",
    "    train_features = make_features(train, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n",
    "    test_features = make_features(test, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "    estimator, model_fn, run_config = estimator_builder(\n",
    "                                  bert_model_hub, \n",
    "                                  OUTPUT_DIR, \n",
    "                                  SAVE_SUMMARY_STEPS, \n",
    "                                  SAVE_CHECKPOINTS_STEPS, \n",
    "                                  label_list, \n",
    "                                  LEARNING_RATE, \n",
    "                                  num_train_steps, \n",
    "                                  num_warmup_steps, \n",
    "                                  BATCH_SIZE)\n",
    "\n",
    "    train_input_fn = run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "    test_input_fn = run_classifier.input_fn_builder(\n",
    "        features=test_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
    "#     result_dict = {}\n",
    "\n",
    "    predict_input_fn = run_classifier.input_fn_builder(\n",
    "        features=test_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "    pred_result = estimator.predict(input_fn=predict_input_fn)\n",
    "    \n",
    "    return result_dict, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(fn):\n",
    "    dataset = []\n",
    "    with open(dev_fn, 'r') as fr:\n",
    "        for l in fr:\n",
    "            label, text = l.strip().split('\\t')\n",
    "            if text.lower().strip() and label:\n",
    "                dataset.append([text.lower().strip(), int(label)])\n",
    "\n",
    "    data_df = pd.DataFrame(data=dataset)\n",
    "    data_df.columns=['comment', 'sentiment']\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(result):\n",
    "    df = pd.DataFrame([result]).T\n",
    "    df.columns = [\"values\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = 'weibo_data/train.tsv'\n",
    "test_fn = 'weibo_data/test.tsv'\n",
    "dev_fn = 'weibo_data/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = transform_data(train_fn)\n",
    "train = train.sample(len(train))\n",
    "\n",
    "test = transform_data(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()\n",
    "# test['sentiment'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- dianping dataset --------#\n",
    "# import pickle\n",
    "# with open('dianping_train_test.pickle', 'rb') as fr:\n",
    "#     train, test = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hub = \"hub_bert_v1\"\n",
    "\n",
    "myparam = {\n",
    "        \"DATA_COLUMN\": \"comment\",\n",
    "        \"LABEL_COLUMN\": \"sentiment\",\n",
    "        \"BATCH_SIZE\": 16,\n",
    "        \"MAX_SEQ_LENGTH\": 128,\n",
    "        \"LEARNING_RATE\": 2e-5,\n",
    "        \"NUM_TRAIN_EPOCHS\": 10,\n",
    "        \"bert_model_hub\": bert_hub\n",
    "    }\n",
    "\n",
    "random.seed(10)\n",
    "OUTPUT_DIR = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, estimator = run_on_dfs(train, test, **myparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretty_print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = train['sentiment'].unique().tolist()\n",
    "\n",
    "# tokenizer = create_tokenizer_from_hub_module(myparam['bert_model_hub'])\n",
    "\n",
    "# test_features = make_features(test, label_list, myparam['MAX_SEQ_LENGTH'], tokenizer, 'comment', 'sentiment')\n",
    "\n",
    "# test_input_fn = run_classifier.input_fn_builder(\n",
    "#     features=test_features,\n",
    "#     seq_length=myparam['MAX_SEQ_LENGTH'],\n",
    "#     is_training=False,\n",
    "#     drop_remainder=False)\n",
    "# result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
    "# result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_list = train['sentiment'].unique().tolist()\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module(myparam['bert_model_hub'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = [['深夜发吃报复社会', 2], ['恨,要报复社会', 1],['尼玛，节操碎了碎了,小姐不可怕，就怕小姐有文化', 1]]\n",
    "\n",
    "predict_df = pd.DataFrame(data=predict_dataset)\n",
    "predict_df.columns=['comment', 'sentiment']\n",
    "\n",
    "# predict_df = transform_data(train_fn)\n",
    "# predict_df = predict_df[predict_df['sentiment'] == 1]\n",
    "\n",
    "predict_features = make_features(predict_df, label_list, myparam['MAX_SEQ_LENGTH'], tokenizer, 'comment', 'sentiment')\n",
    "\n",
    "predict_input_fn = run_classifier.input_fn_builder(\n",
    "        features=predict_features,\n",
    "        seq_length=myparam['MAX_SEQ_LENGTH'],\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "result_dict = estimator.predict(input_fn=predict_input_fn, yield_single_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(result_dict)\n",
    "\n",
    "# for (i, prediction) in enumerate(result_dict):\n",
    "#     probabilities = prediction[\"probabilities\"]\n",
    "#     print(type(probabilities), probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_df = transform_data(dev_fn)\n",
    "\n",
    "predict_labels = predict_df['sentiment']\n",
    "predict_text = predict_df['comment']\n",
    "\n",
    "predict_features = make_features(predict_df, label_list, myparam['MAX_SEQ_LENGTH'], tokenizer, 'comment', 'sentiment')\n",
    "\n",
    "predict_input_fn = run_classifier.input_fn_builder(\n",
    "        features=predict_features,\n",
    "        seq_length=myparam['MAX_SEQ_LENGTH'],\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)\n",
    "\n",
    "output_predict_file = \"test_results.tsv\"\n",
    "num_actual_predict_examples = len(predict_df)\n",
    "with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
    "    tf.logging.info(\"***** Predict results *****\")\n",
    "    for (i, prediction) in enumerate(result):\n",
    "        probabilities = prediction[\"probabilities\"]\n",
    "        if i >= num_actual_predict_examples:\n",
    "            break\n",
    "        output_line = \"\\t\".join(str(class_probability) for class_probability in probabilities) + '\\t' + str(predict_labels[i]) + '\\t' + predict_text[i] + \"\\n\"\n",
    "        writer.write(output_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\n",
    "    input_ids = tf.placeholder(tf.int32, [None, myparam['MAX_SEQ_LENGTH']], name='input_ids')\n",
    "    input_mask = tf.placeholder(tf.int32, [None, myparam['MAX_SEQ_LENGTH']], name='input_mask')\n",
    "    segment_ids = tf.placeholder(tf.int32, [None, myparam['MAX_SEQ_LENGTH']], name='segment_ids')\n",
    "\n",
    "    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({'label_ids': label_ids,'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids})()\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = os.path.join(OUTPUT_DIR, 'exported')\n",
    "estimator._export_to_tpu = False\n",
    "estimator.export_savedmodel(export_dir, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
